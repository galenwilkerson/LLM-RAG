{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f09c6f7",
   "metadata": {},
   "source": [
    "# LLM RAG example\n",
    "\n",
    "**There are things an LLM doesn't know and must look up.**\n",
    "\n",
    "More here:\n",
    "\n",
    "https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f373ea6",
   "metadata": {},
   "source": [
    "## High-Level Intuition of Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a **method that improves how AI models answer questions by looking up relevant information before generating a response.**\n",
    "\n",
    "1. **Retriever**: Finds relevant information from a large collection of documents based on the question asked.\n",
    "2. **Generator**: Uses both the question and the found information to create a more accurate and informative answer.\n",
    "\n",
    "**Why RAG?**\n",
    "- **Better Answers**: By checking relevant information first, the AI gives more accurate and useful responses.\n",
    "\n",
    "**How It Works?**\n",
    "1. **Ask a Question**: You provide a question.\n",
    "2. **Look Up Information**: The model searches for related information from a database.\n",
    "3. **Generate Answer**: The model uses the found information to generate a well-informed answer.\n",
    "\n",
    "In short, RAG combines looking up information and generating text to give better answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b787056",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "## Retrieval-Augmented Generation (RAG) Overview\n",
    "\n",
    "### RAG combines two components:\n",
    "\n",
    "    Retriever: Retrieves relevant documents from a predefined knowledge base.\n",
    "    Generator: Generates text based on the retrieved documents.\n",
    "\n",
    "### Steps to Implement RAG:\n",
    "\n",
    "    Load a Pre-trained Language Model and a Retriever: Use a small language model and a retriever model.\n",
    "    Create a Knowledge Base: Prepare a simple corpus of documents.\n",
    "    Retrieve Relevant Documents: Use the retriever to find documents relevant to the input query.\n",
    "    Generate a Response: Use the language model to generate a response based on the retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42cd49",
   "metadata": {},
   "source": [
    "## Explanation of the Code\n",
    "\n",
    "The provided code demonstrates a simple implementation of a Retrieval-Augmented Generation (RAG) system using smaller pre-trained models from the Hugging Face Transformers library. It involves four main steps:\n",
    "\n",
    "#### 1. Install Necessary Packages\n",
    "The code begins by installing required libraries: `transformers`, `datasets`, and `sentencepiece`. These libraries provide tools for working with pre-trained language models and datasets, and `sentencepiece` is specifically used for tokenization with the T5 model.\n",
    "\n",
    "#### 2. Import Libraries\n",
    "Next, the code imports necessary modules from the `transformers` library, as well as standard Python libraries for numerical operations and tensor handling. The transformers library provides access to various pre-trained models and tokenizers.\n",
    "\n",
    "#### 3. Load Pre-trained Models and Tokenizers\n",
    "The script loads pre-trained models and tokenizers for both the retriever and the generator components:\n",
    "- **Retriever**: Utilizes `DistilBERT`, a smaller and faster variant of BERT, for retrieving relevant documents. The `DistilBertTokenizer` tokenizes input text for the `DistilBertModel`, which generates embeddings for the input text.\n",
    "- **Generator**: Uses `T5`, a model designed for text generation tasks. The `T5Tokenizer` tokenizes input text for the `T5ForConditionalGeneration` model, which generates text responses based on the input context.\n",
    "\n",
    "#### 4. Create a Knowledge Base\n",
    "The script creates a simple knowledge base, which is a list of documents containing basic factual sentences. These documents are encoded using the retriever model to create embeddings that represent the content of each document.\n",
    "\n",
    "#### 5. Retrieve Relevant Documents\n",
    "A function `retrieve_docs` is defined to retrieve relevant documents from the knowledge base based on a given query. The function:\n",
    "- Encodes the query using the retriever tokenizer and model.\n",
    "- Computes the similarity between the query embeddings and the document embeddings using dot products.\n",
    "- Retrieves the top k most relevant documents based on the computed similarity scores.\n",
    "\n",
    "#### 6. Generate a Response\n",
    "Another function `generate_response` is defined to generate a text response based on the retrieved documents. The function:\n",
    "- Retrieves relevant documents using the `retrieve_docs` function.\n",
    "- Concatenates the retrieved documents to form a context.\n",
    "- Tokenizes the query and context using the T5 tokenizer.\n",
    "- Generates a response using the T5 model based on the tokenized input.\n",
    "- Decodes the generated response from token IDs back to text.\n",
    "\n",
    "#### Example Usage\n",
    "The script includes an example usage section where a query (\"What color is the sky?\") is processed to generate a response. The query is passed to the `generate_response` function, which retrieves relevant documents from the knowledge base and generates an informative response using the T5 model. The response is then printed out, demonstrating the RAG system in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79b6009",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What color is the sky?\n",
      "Response: blue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galen/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "#!pip install transformers datasets sentencepiece\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# 1. Load Smaller Pre-trained Models and Tokenizers\n",
    "# Using DistilBERT for the retriever\n",
    "retriever_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "retriever_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Using T5 for the generator\n",
    "generator_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "generator_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# 2. Create a Knowledge Base\n",
    "documents = [\n",
    "    \"The sky is blue.\",\n",
    "    \"The sun is bright.\",\n",
    "    \"There are many stars in the universe.\",\n",
    "    \"AI is transforming the world.\"\n",
    "]\n",
    "\n",
    "# Encode the documents using the retriever\n",
    "contexts = retriever_tokenizer(documents, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "context_embeddings = retriever_model(**contexts).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# 3. Retrieve Relevant Documents\n",
    "def retrieve_docs(query, top_k=1):\n",
    "    # Encode the query using the retriever\n",
    "    query_inputs = retriever_tokenizer(query, return_tensors=\"pt\")\n",
    "    query_embeddings = retriever_model(**query_inputs).last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    # Compute dot product between query and context embeddings\n",
    "    scores = torch.matmul(query_embeddings, context_embeddings.T)\n",
    "    \n",
    "    # Retrieve top k documents\n",
    "    top_k_indices = torch.topk(scores, k=top_k, dim=-1).indices.squeeze().tolist()\n",
    "    if isinstance(top_k_indices, int):\n",
    "        top_k_indices = [top_k_indices]\n",
    "    return [documents[i] for i in top_k_indices]\n",
    "\n",
    "# 4. Generate a Response\n",
    "def generate_response(query):\n",
    "    retrieved_docs = retrieve_docs(query)\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    \n",
    "    # Tokenize input for T5\n",
    "    inputs = generator_tokenizer(f\"question: {query} context: {context}\", return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate response\n",
    "    output = generator_model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], num_return_sequences=1)\n",
    "    \n",
    "    return generator_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "query = \"What color is the sky?\"\n",
    "response = generate_response(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561cf626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
